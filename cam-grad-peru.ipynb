{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pyha_analyzer.dataset import get_datasets, make_dataloaders, PyhaDFDataset, config\n",
    "import wandb\n",
    "import torch\n",
    "from pyha_analyzer.models.timm_model import TimmModel\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.amp.autocast_mode import autocast\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import MultilabelAveragePrecision\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from pyha_analyzer import config\n",
    "from pyha_analyzer.dataset import get_datasets, make_dataloaders, PyhaDFDataset\n",
    "from pyha_analyzer.utils import set_seed\n",
    "from pyha_analyzer.models.early_stopper import EarlyStopper\n",
    "from pyha_analyzer.models.timm_model import TimmModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchaudio import transforms as audtr\n",
    "cfg = config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = \"./models/eca_nfnet_l0-20240419-0202.pt\"\n",
    "model_for_run = TimmModel(num_classes=132, \n",
    "                            model_name=cfg.model).to(cfg.device)\n",
    "model_for_run.load_state_dict(torch.load(weights))\n",
    "model = model_for_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_mel = audtr.MelSpectrogram(\n",
    "        sample_rate=cfg.sample_rate,\n",
    "        n_mels=cfg.n_mels,\n",
    "        n_fft=cfg.n_fft).to(cfg.prepros_device)\n",
    "decibel_convert = audtr.AmplitudeToDB(stype=\"power\").to(cfg.prepros_device)\n",
    "\n",
    "def to_image( audio):\n",
    "        \"\"\"\n",
    "        Convert audio clip to 3-channel spectrogram image\n",
    "        \"\"\"\n",
    "        # Mel spectrogram\n",
    "        # Pylint complains this is not callable, but it is a torch.nn.Module\n",
    "        # pylint: disable-next=not-callable\n",
    "        mel = convert_to_mel(audio)\n",
    "        # Convert to decibels\n",
    "        # pylint: disable-next=not-callable\n",
    "        mel = decibel_convert(mel)\n",
    "        # Convert to Image\n",
    "        \n",
    "        # Normalize Image (https://medium.com/@hasithsura/audio-classification-d37a82d6715)\n",
    "        mean = mel.mean()\n",
    "        std = mel.std()\n",
    "        mel = (mel - mean) / (std + 1e-6)\n",
    "        \n",
    "        # Sigmoid to get 0 to 1 scaling (0.5 becomes mean)\n",
    "        mel = torch.sigmoid(mel)\n",
    "        return torch.stack([mel, mel, mel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from pyha_analyzer import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GRABADOR-SDZG-AM-1_PIHA_single_w_confidences_chunked.csv\")\n",
    "df[\"FILE NAME\"] =  df[\"IN FILE\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(idx = 0):\n",
    "    annotation = df.iloc[idx]\n",
    "\n",
    "\n",
    "    audio, sample_rate = torchaudio.load(       #pyright: ignore [reportGeneralTypeIssues ]\n",
    "        os.path.join(\"//e4e-nas.ucsd.edu/passive-acoustic-biodiversity/Peru_2019_Audiomoth_Sound_Recordings/\", annotation[\"IN FILE\"])\n",
    "    )\n",
    "\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = utils.to_mono(audio)\n",
    "\n",
    "    # Resample\n",
    "    if sample_rate != cfg.sample_rate:\n",
    "        resample = audtr.Resample(sample_rate, cfg.sample_rate)\n",
    "        audio = resample(audio)\n",
    "\n",
    "    frame_offset = int(annotation[cfg.offset_col] * cfg.sample_rate)\n",
    "    print(frame_offset)\n",
    "\n",
    "    num_frames = int(annotation[cfg.duration_col] * cfg.sample_rate)\n",
    "\n",
    "    audio = audio[frame_offset: frame_offset+num_frames]\n",
    "\n",
    "    # Crop if too long\n",
    "    target_num_samples = 5 * cfg.sample_rate\n",
    "    if audio.shape[0] > 5 * cfg.sample_rate:\n",
    "        audio = utils.crop_audio(audio, target_num_samples)\n",
    "    # Pad if too short\n",
    "    if audio.shape[0] < 5 * cfg.sample_rate:\n",
    "        audio = utils.pad_audio(audio, target_num_samples)\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_last_layer_activation(audio):\n",
    "    target_layers = [model.model.final_conv]\n",
    "    img = to_image(audio)\n",
    "    input_tensor = torch.unsqueeze(img, 0).to(\"cpu\")# Create an input tensor image for your model..\n",
    "    # Note: input_tensor can be a batch tensor with several images!\n",
    "\n",
    "    # Construct the CAM object once, and then re-use it on many images:\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    cam2 = GradCAMPlusPlus(model=model, target_layers=target_layers)\n",
    "\n",
    "    targets = None#[ClassifierOutputTarget(64)]\n",
    "\n",
    "    # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    grayscale_cam2 = cam2(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "    # In this example grayscale_cam has only one image in the batch:\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    visualization = show_cam_on_image(img.permute(1, 2, 0).numpy(), grayscale_cam, use_rgb=True)\n",
    "    grayscale_cam2 = grayscale_cam2[0, :]\n",
    "    visualization2 = show_cam_on_image(img.permute(1, 2, 0).numpy(), grayscale_cam2, use_rgb=True)\n",
    "\n",
    "    # You can also get the model outputs without having to re-inference\n",
    "    model_outputs = cam.outputs\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(8, 16))  # 2 rows of subplots\n",
    "\n",
    "    # Plot data on the first subplot\n",
    "    axs[0].imshow(visualization)\n",
    "    axs[0].set_title('GradCAM')\n",
    "    axs[0].invert_yaxis()\n",
    "\n",
    "    axs[1].imshow(visualization2)\n",
    "    axs[1].set_title('GradCAM++')\n",
    "    axs[1].invert_yaxis()\n",
    "\n",
    "    # Plot data on the second subplot\n",
    "    axs[2].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axs[2].set_title(\"Original Image\")\n",
    "    axs[2].invert_yaxis()\n",
    "    \n",
    "    plt.show()\n",
    "    return grayscale_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df[df[\"MANUAL ID\"] == \"PIHA_1\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "for i in df[df[\"MANUAL ID\"] == \"PIHA_1\"].drop_duplicates().index:\n",
    "    print(i)\n",
    "    audio = preprocess_audio(i)\n",
    "    show_last_layer_activation(audio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
